# 02. Metin Temsili (Text Representation)

## ğŸ¯ Metin Temsili YolculuÄŸu

<p align="center">
  <b>Ham metinlerden sayÄ±sal vektÃ¶rlere: NLP modellerinin anlayabileceÄŸi dile dÃ¶nÃ¼ÅŸÃ¼m!</b>
</p>

```mermaid
flowchart TD
    style A1 fill:#D6EAF8,stroke:#2980B9,stroke-width:2px
    style B1 fill:#F9E79F,stroke:#B7950B,stroke-width:2px
    style B2 fill:#D5F5E3,stroke:#229954,stroke-width:2px
    style B3 fill:#FADBD8,stroke:#C0392B,stroke-width:2px
    style B4 fill:#E8DAEF,stroke:#8E44AD,stroke-width:2px
    style B5 fill:#D4E6F1,stroke:#2471A3,stroke-width:2px
    style Z1 fill:#D5DBDB,stroke:#34495E,stroke-width:2px

    A1([Ham Metin])
    B1([Bag of Words<br><i>Kelime SÄ±klÄ±ÄŸÄ±</i>])
    B2([N-Grams<br><i>Kelime Dizileri</i>])
    B3([TF-IDF<br><i>Kelime Ã–nemi</i>])
    B4([Word Embedding<br><i>Anlam VektÃ¶rleri</i>])
    B5([Transformers<br><i>BaÄŸlamsal Temsil</i>])
    Z1([Derin Ã–ÄŸrenme Modelleri])

    A1 --> B1
    B1 --> B2
    B2 --> B3
    B3 --> B4
    B4 --> B5
    B5 --> Z1

    B1 -.-> B3
    B2 -.-> B4
    B3 -.-> B5
```

---

## ğŸ“Š Proje Ã–zeti

Bu klasÃ¶rde, **NLP'de metin temsili yÃ¶ntemleri** sÄ±fÄ±rdan ileri seviyeye kadar profesyonel ÅŸekilde uygulanmaktadÄ±r.  
Ã‡alÄ±ÅŸmalar, **IMDB film yorumlarÄ±** ve **SMS spam** veri setleri Ã¼zerinde, endÃ¼stri standardÄ± Python kÃ¼tÃ¼phaneleri (scikit-learn, gensim, transformers, vb.) ve modern NLP teknikleriyle gerÃ§ekleÅŸtirilmiÅŸtir.

### Ana Konular:
- Bag of Words (BoW) ile temel metin vektÃ¶rleÅŸtirme
- N-Grams ile kelime dizilerini yakalama
- TF-IDF ile kelimelerin Ã¶nem derecelerini hesaplama
- Word Embedding ile anlamsal kelime vektÃ¶rleri
- Transformers ile baÄŸlamsal metin temsili

---

## ğŸŒŸ Metin Temsili YÃ¶ntemleri & Flashcardlar

### 1. **Bag of Words (BoW)**
- **AmaÃ§:** Metni kelime sÄ±klÄ±klarÄ±na gÃ¶re sayÄ±sal vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rmek.
- **Ã–zellikler:**
  - Kelime sÄ±rasÄ± dikkate alÄ±nmaz
  - Basit ve hÄ±zlÄ±
  - Seyrek (sparse) matris Ã¼retir
- **KullanÄ±m AlanlarÄ±:** Metin sÄ±nÄ±flandÄ±rma, spam tespiti, duygu analizi
- **Kod:**
  ```python
  from sklearn.feature_extraction.text import CountVectorizer
  
  corpus = ["NLP Ã§ok gÃ¼Ã§lÃ¼", "NLP ile metin analizi", "Metin madenciliÄŸi harika"]
  vectorizer = CountVectorizer()
  X = vectorizer.fit_transform(corpus)
  
  print(vectorizer.get_feature_names_out())
  print(X.toarray())
  # Ã‡Ä±ktÄ±: Her kelime iÃ§in sÄ±klÄ±k deÄŸerleri
  ```
- <div style="border:1px solid #B7950B; border-radius:8px; padding:12px; background:#FFFBEA; margin:10px 0;">
  <b>Soru:</b> BoW yÃ¶nteminin en bÃ¼yÃ¼k dezavantajÄ± nedir?<br>
  <b>Cevap:</b> Kelime sÄ±rasÄ±nÄ± ve baÄŸlamÄ± gÃ¶z ardÄ± eder. "Kedi kÃ¶peÄŸi kovaladÄ±" ile "KÃ¶pek kediyi kovaladÄ±" aynÄ± vektÃ¶rle temsil edilir.
  </div>

---

### 2. **N-Grams**
- **AmaÃ§:** ArdÄ±ÅŸÄ±k n kelimelik gruplarÄ± yakalayarak kelimelerin birlikte kullanÄ±mÄ±nÄ± modellemek.
- **Ã–zellikler:**
  - Unigram (1 kelime), Bigram (2 kelime), Trigram (3 kelime)
  - Kelime sÄ±rasÄ±nÄ± kÄ±smen korur
  - Daha zengin Ã¶zellik uzayÄ±
- **KullanÄ±m AlanlarÄ±:** Dil modelleme, otomatik tamamlama, anahtar kelime Ã§Ä±karÄ±mÄ±
- **Kod:**
  ```python
  from sklearn.feature_extraction.text import CountVectorizer
  
  corpus = ["makine Ã¶ÄŸrenmesi Ã§ok eÄŸlenceli", "derin Ã¶ÄŸrenme NLP iÃ§in gÃ¼Ã§lÃ¼"]
  
  # Bigram (2-gram) oluÅŸturma
  vectorizer = CountVectorizer(ngram_range=(2, 2))
  X = vectorizer.fit_transform(corpus)
  
  print(vectorizer.get_feature_names_out())
  # Ã‡Ä±ktÄ±: ['derin Ã¶ÄŸrenme', 'iÃ§in gÃ¼Ã§lÃ¼', 'makine Ã¶ÄŸrenmesi', 'nlp iÃ§in', 'Ã¶ÄŸrenme nlp', 'Ã¶ÄŸrenmesi Ã§ok', 'Ã§ok eÄŸlenceli']
  ```
- <div style="border:1px solid #229954; border-radius:8px; padding:12px; background:#F0FDF4; margin:10px 0;">
  <b>Soru:</b> N-gram boyutu arttÄ±kÃ§a ne gibi bir problem ortaya Ã§Ä±kar?<br>
  <b>Cevap:</b> Ã–zellik uzayÄ± Ã§ok bÃ¼yÃ¼r ve seyreklik (sparsity) artar. Bu da hesaplama maliyetini artÄ±rÄ±r ve overfitting riskini yÃ¼kseltir.
  </div>

---

### 3. **TF-IDF (Term Frequency - Inverse Document Frequency)**
- **AmaÃ§:** Kelimelerin Ã¶nem derecelerini hesaplayarak ayÄ±rt edici kelimeleri Ã¶ne Ã§Ä±karmak.
- **FormÃ¼l:**
  - **TF (Term Frequency):** Bir kelimenin dokÃ¼mandaki sÄ±klÄ±ÄŸÄ±
  - **IDF (Inverse Document Frequency):** Kelimenin nadir bulunma derecesi
  - **TF-IDF = TF Ã— IDF**
- **Ã–zellikler:**
  - Her dokÃ¼manda sÄ±k geÃ§en kelimeler (Ã¶r: "ve", "ama") dÃ¼ÅŸÃ¼k skor alÄ±r
  - Nadir ve ayÄ±rt edici kelimeler yÃ¼ksek skor alÄ±r
- **KullanÄ±m AlanlarÄ±:** Anahtar kelime Ã§Ä±karÄ±mÄ±, dokÃ¼man benzerliÄŸi, bilgi eriÅŸimi
- **Kod:**
  ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  
  corpus = [
      "KÃ¶pek Ã§ok tatlÄ± bir hayvandÄ±r",
      "Kedi Ã§ok tatlÄ± hayvanlardÄ±r",
      "Ä°nekler sÃ¼t Ã¼retirler",
      "KuÅŸlar gÃ¶Ã§ eder"
  ]
  
  vectorizer = TfidfVectorizer()
  X = vectorizer.fit_transform(corpus)
  
  print(vectorizer.get_feature_names_out())
  print(X.toarray())
  # Ã‡Ä±ktÄ±: Her kelime iÃ§in TF-IDF skorlarÄ±
  ```
- <div style="border:1px solid #C0392B; border-radius:8px; padding:12px; background:#FEF5F5; margin:10px 0;">
  <b>Soru:</b> TF-IDF'in BoW'a gÃ¶re avantajÄ± nedir?<br>
  <b>Cevap:</b> TF-IDF, sÄ±k geÃ§en ama anlamsÄ±z kelimelerin etkisini azaltÄ±r ve ayÄ±rt edici kelimelere daha fazla aÄŸÄ±rlÄ±k verir. Bu sayede daha anlamlÄ± Ã¶zellikler elde edilir.
  </div>

---

### 4. **Word Embedding (Kelime GÃ¶mme)**
- **AmaÃ§:** Kelimeleri yoÄŸun (dense) ve anlamsal olarak zengin vektÃ¶rlerle temsil etmek.
- **PopÃ¼ler YÃ¶ntemler:**
  - **Word2Vec (Google):** CBOW ve Skip-gram modelleri
  - **FastText (Facebook/Meta):** Karakter n-gramlarÄ±nÄ± kullanÄ±r
  - **GloVe (Stanford):** KÃ¼resel kelime eÅŸ-oluÅŸum matrisi
- **Ã–zellikler:**
  - Anlamsal benzerlik yakalar (Ã¶r: "kral" - "kraliÃ§e" â‰ˆ "adam" - "kadÄ±n")
  - DÃ¼ÅŸÃ¼k boyutlu yoÄŸun vektÃ¶rler (genelde 100-300 boyut)
  - Transfer learning iÃ§in kullanÄ±labilir
- **KullanÄ±m AlanlarÄ±:** Metin sÄ±nÄ±flandÄ±rma, duygu analizi, makine Ã§evirisi, soru-cevap sistemleri
- **Kod:**
  ```python
  from gensim.models import Word2Vec
  
  # Ã–rnek cÃ¼mleler
  sentences = [
      ["makine", "Ã¶ÄŸrenmesi", "Ã§ok", "eÄŸlenceli"],
      ["derin", "Ã¶ÄŸrenme", "nlp", "iÃ§in", "gÃ¼Ã§lÃ¼"],
      ["yapay", "zeka", "geleceÄŸin", "teknolojisi"]
  ]
  
  # Word2Vec modelini eÄŸit
  model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
  
  # Benzer kelimeleri bul
  similar = model.wv.most_similar("Ã¶ÄŸrenme", topn=3)
  print(similar)
  
  # Kelime vektÃ¶rÃ¼
  vector = model.wv["makine"]
  print(vector.shape)  # (100,)
  ```
- <div style="border:1px solid #8E44AD; border-radius:8px; padding:12px; background:#F9F5FC; margin:10px 0;">
  <b>Soru:</b> Word2Vec ile FastText arasÄ±ndaki temel fark nedir?<br>
  <b>Cevap:</b> Word2Vec kelimeleri tek birim olarak iÅŸlerken, FastText kelimeleri karakter n-gramlarÄ±na bÃ¶ler. Bu sayede FastText, sÃ¶zlÃ¼kte olmayan (OOV) kelimeler iÃ§in de vektÃ¶r Ã¼retebilir.
  </div>

---

### 5. **Transformers (BaÄŸlamsal Metin Temsili)**
- **AmaÃ§:** Her kelimenin baÄŸlamÄ±na gÃ¶re dinamik vektÃ¶r temsili oluÅŸturmak.
- **PopÃ¼ler Modeller:**
  - **BERT (Bidirectional Encoder Representations from Transformers)**
  - **GPT (Generative Pre-trained Transformer)**
  - **RoBERTa, ALBERT, DistilBERT**
  - **TÃ¼rkÃ§e Modeller:** BERTurk, mBERT
- **Ã–zellikler:**
  - Attention mekanizmasÄ± ile baÄŸlamsal anlam
  - Transfer learning (Ã¶n eÄŸitimli modeller)
  - Son teknoloji (state-of-the-art) performans
  - AynÄ± kelime farklÄ± cÃ¼mlelerde farklÄ± vektÃ¶rlerle temsil edilir
- **KullanÄ±m AlanlarÄ±:** TÃ¼m NLP gÃ¶revleri (sÄ±nÄ±flandÄ±rma, NER, soru-cevap, Ã¶zetleme, vb.)
- **Kod:**
  ```python
  from transformers import AutoTokenizer, AutoModel
  import torch
  
  # Model ve tokenizer yÃ¼kle
  model_name = "bert-base-uncased"
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModel.from_pretrained(model_name)
  
  # Metin
  text = "Transformers are powerful for NLP"
  
  # Tokenize et
  inputs = tokenizer(text, return_tensors="pt")
  
  # Model Ã§Ä±ktÄ±sÄ±
  with torch.no_grad():
      outputs = model(**inputs)
  
  # Son katman gizli durumlarÄ± (hidden states)
  embeddings = outputs.last_hidden_state
  print(embeddings.shape)  # torch.Size([1, sequence_length, 768])
  ```
- <div style="border:1px solid #2471A3; border-radius:8px; padding:12px; background:#F0F8FF; margin:10px 0;">
  <b>Soru:</b> BERT'in Word2Vec'e gÃ¶re en Ã¶nemli avantajÄ± nedir?<br>
  <b>Cevap:</b> BERT baÄŸlamsal (contextual) temsil oluÅŸturur. Yani "elma" kelimesi "meyve elma" ve "gÃ¶z bebeÄŸi elma" cÃ¼mlelerinde farklÄ± vektÃ¶rlerle temsil edilir. Word2Vec'te ise her kelime iÃ§in sabit bir vektÃ¶r vardÄ±r.
  </div>

---

## ğŸ“‚ KlasÃ¶r Ä°Ã§eriÄŸi

- `01. bow-bagofwords.ipynb` : Bag of Words yÃ¶ntemi ile metin vektÃ¶rleÅŸtirme ve IMDB veri seti Ã¼zerinde sÄ±nÄ±flandÄ±rma
- `02. n-grams.ipynb` : N-Gram kavramÄ± ve uygulamalarÄ± (unigram, bigram, trigram)
- `03. tf_idf.ipynb` : TF-IDF ile kelime Ã¶nem skorlarÄ± ve anahtar kelime Ã§Ä±karÄ±mÄ±
- `04_word_embedding.ipynb` : Word2Vec, FastText ve GloVe ile kelime gÃ¶mme yÃ¶ntemleri
- `05. transformers.ipynb` : BERT ve Transformers ile baÄŸlamsal metin temsili
- `IMDB Dataset.csv` : IMDB film yorumlarÄ± veri seti (duygu analizi)
- `sms_spam.csv` : SMS spam tespiti veri seti

---

## ğŸ”„ YÃ¶ntemler ArasÄ± KarÅŸÄ±laÅŸtÄ±rma

| YÃ¶ntem | Boyut | YoÄŸunluk | BaÄŸlamsal | HÄ±z | KullanÄ±m KolaylÄ±ÄŸÄ± |
|--------|-------|----------|-----------|-----|-------------------|
| **BoW** | Ã‡ok YÃ¼ksek | Seyrek | âŒ | âš¡âš¡âš¡ | â­â­â­ |
| **N-Grams** | Ã‡ok YÃ¼ksek | Seyrek | KÄ±smen | âš¡âš¡ | â­â­â­ |
| **TF-IDF** | Ã‡ok YÃ¼ksek | Seyrek | âŒ | âš¡âš¡âš¡ | â­â­â­ |
| **Word Embedding** | DÃ¼ÅŸÃ¼k (100-300) | YoÄŸun | âŒ | âš¡âš¡ | â­â­ |
| **Transformers** | Orta (768) | YoÄŸun | âœ… | âš¡ | â­ |

---

## ğŸš€ Hangi YÃ¶ntemi Ne Zaman KullanmalÄ±?

### **BoW / TF-IDF**
- âœ… Basit ve hÄ±zlÄ± prototipleme
- âœ… SÄ±nÄ±rlÄ± veri ve hesaplama kaynaÄŸÄ±
- âœ… Metin sÄ±nÄ±flandÄ±rma, spam tespiti
- âŒ BaÄŸlamsal anlam gerekli deÄŸilse

### **N-Grams**
- âœ… Kelime sÄ±rasÄ± Ã¶nemli
- âœ… Dil modelleme, otomatik tamamlama
- âœ… Anahtar kelime Ã§Ä±karÄ±mÄ±
- âŒ Ã‡ok bÃ¼yÃ¼k veri setlerinde dikkatli kullanÄ±lmalÄ±

### **Word Embedding**
- âœ… Anlamsal benzerlik Ã¶nemli
- âœ… Orta Ã¶lÃ§ekli veri setleri
- âœ… Transfer learning ile Ã¶n eÄŸitimli modeller
- âŒ BaÄŸlamsal anlam gerekiyorsa Transformers tercih edilmeli

### **Transformers**
- âœ… En yÃ¼ksek performans gerekli
- âœ… BaÄŸlamsal anlam kritik
- âœ… Yeterli hesaplama kaynaÄŸÄ± mevcut
- âœ… Transfer learning ile hÄ±zlÄ± baÅŸlangÄ±Ã§
- âŒ YÃ¼ksek hesaplama maliyeti

---

## ğŸ’¡ Kaynaklar

- [scikit-learn Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)
- [Gensim Word2Vec Documentation](https://radimrehurek.com/gensim/models/word2vec.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
- [Stanford NLP Course](https://web.stanford.edu/class/cs224n/)
- [FastText Documentation](https://fasttext.cc/)

---

## ğŸ“ Ã–ÄŸrenme Yol HaritasÄ±

```mermaid
graph LR
    A[Metin Ã–n Ä°ÅŸleme] --> B[BoW & TF-IDF]
    B --> C[N-Grams]
    C --> D[Word Embedding]
    D --> E[Transformers]
    E --> F[Ä°leri DÃ¼zey NLP]
    
    style A fill:#D6EAF8
    style B fill:#F9E79F
    style C fill:#D5F5E3
    style D fill:#FADBD8
    style E fill:#E8DAEF
    style F fill:#D5DBDB
```

---

> **Metin temsili, NLP'nin kalbidir. DoÄŸru temsil yÃ¶ntemi seÃ§imi, model baÅŸarÄ±sÄ±nÄ±n %80'ini belirler!**

---

## ğŸ“ Notlar

- TÃ¼m notebook'larda **adÄ±m adÄ±m aÃ§Ä±klamalar** ve **flashcard** tarzÄ± Ã¶ÄŸrenme notlarÄ± bulunmaktadÄ±r
- Her yÃ¶ntem iÃ§in **gerÃ§ek veri setleri** kullanÄ±lmÄ±ÅŸtÄ±r
- **GÃ¶rselleÅŸtirmeler** ve **karÅŸÄ±laÅŸtÄ±rmalar** ile kavramlar pekiÅŸtirilmiÅŸtir
- **Best practices** ve **production tips** eklenmiÅŸtir